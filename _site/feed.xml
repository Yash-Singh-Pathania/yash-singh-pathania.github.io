<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-09-03T10:21:11+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Hii There</title><subtitle>Hey there! I‚Äôm Yash, a Ruby and Python developer with a knack for AWS, diving into computer vision, and pioneering AR/VR. I love blending tech with a dash of sci-fi, turning futuristic dreams into reality. From backend architectures to immersive AR experiences, I‚Äôm all about exploring new dimensions of code and pushing the boundaries of what‚Äôs possible.</subtitle><author><name>Yash</name><email>yashpathania704@gmail.com</email></author><entry><title type="html">Bruno</title><link href="http://localhost:4000/posts/bruno/" rel="alternate" type="text/html" title="Bruno" /><published>2024-09-03T00:00:00+01:00</published><updated>2024-09-03T00:00:00+01:00</updated><id>http://localhost:4000/posts/Bruno</id><content type="html" xml:base="http://localhost:4000/posts/bruno/"><![CDATA[<p>APIs are crucial today, but testing tools  inlcuding dev testing especially between devs is a hassle. The most popular tool, Postman, has become bloated, with many features hidden behind a paywall. This led me on a search for an open-source, privacy-focused API testing tool‚Äî<strong>Bruno</strong>.</p>

<h2 id="why-bruno-">Why Bruno? ü§î</h2>

<p>Bruno lets users store collections and API requests, traditionally kept in Postman, but with a more streamlined, Git-friendly approach. This eliminates the confusion of having API requests versioned separately from your code, making it easier to share cURL requests across teams like QA, product, and other stakeholders.</p>

<p><img src="https://miro.medium.com/v2/resize:fit:1228/1*CHz8MRsvJXszYsUTehVZkQ.png" alt="Bruno Logo" /></p>

<h3 id="personal-story-time-">Personal Story Time! üìñ</h3>

<p>Let me tell you about the time I first tried Bruno. I was working on a hackathon project at an internal event at <a href="https://www.1mg.com">1mg</a>, and the APIs were a tangled mess. Our team struggled with complex collections and bloated workspaces. Then, I found Bruno. It felt like discovering a secret weapon. Suddenly, APIs were fun again! I could clone a repo, see real examples, and get to work immediately. No more ‚ÄúWhere‚Äôs that JSON file?‚Äù moments. üí°</p>

<h2 id="bruno-vs-other-api-tools-">Bruno vs. Other API Tools üîç</h2>

<h3 id="key-benefits">Key Benefits:</h3>
<ul>
  <li>
    <p><strong>Lightweight &amp; User-Friendly Interface</strong>: Bruno‚Äôs snappy UI is built with Electron and available on all major desktop platforms‚ÄîLinux, Mac, and Windows. Unlike Postman, Bruno is entirely offline, ensuring your data stays on your machine.</p>
  </li>
  <li>
    <p><strong>Privacy First</strong>: Unlike Postman, which routes API requests through proprietary proxy servers, Bruno handles requests directly from your computer, keeping your data private and secure.</p>
  </li>
  <li>
    <p><strong>Git Integration</strong>: Bruno uses a plain text markup language called Bru, storing API requests directly in your repository. This allows seamless version control and collaboration using tools like Git.</p>
  </li>
</ul>

<h3 id="additional-features">Additional Features:</h3>
<ul>
  <li><strong>No Paywall</strong>: Bruno is a FOSS project with no hidden fees, unlike Postman, which locks essential features behind a paywall.</li>
  <li><strong>Unlimited Collection Runs</strong>: Postman limits collection runs, but with Bruno, you can run collections as many times as you like.</li>
  <li><strong>Secret Management</strong>: Load secrets directly from a <code class="language-plaintext highlighter-rouge">.env</code> file, eliminating the need to manage them separately.</li>
  <li><strong>Declarative Scripting</strong>: Write tests and update post-response variables using simple expressions, supported by Bruno‚Äôs declarative scripting features.</li>
  <li><strong>CLI Support</strong>: Run API collections with ease using Bruno‚Äôs CLI, integrating smoothly with your CI/CD workflows.</li>
</ul>

<h3 id="the-future-of-api-testing-Ô∏è">The Future of API Testing üõ†Ô∏è</h3>

<p>Bruno is revolutionizing API testing by focusing on privacy, efficiency, and user control. By co-locating API collections with your source code and enabling Git management, Bruno offers a cohesive and secure solution for developers and teams.</p>]]></content><author><name>Yash</name><email>yashpathania704@gmail.com</email></author><category term="CodingTools" /><summary type="html"><![CDATA[APIs are crucial today, but testing tools inlcuding dev testing especially between devs is a hassle. The most popular tool, Postman, has become bloated, with many features hidden behind a paywall. This led me on a search for an open-source, privacy-focused API testing tool‚ÄîBruno.]]></summary></entry><entry><title type="html">Hawkeye : Computer Vision For Metrics</title><link href="http://localhost:4000/posts/building_hawkeye/" rel="alternate" type="text/html" title="Hawkeye : Computer Vision For Metrics" /><published>2024-08-14T00:00:00+01:00</published><updated>2024-08-14T00:00:00+01:00</updated><id>http://localhost:4000/posts/building_hawkeye</id><content type="html" xml:base="http://localhost:4000/posts/building_hawkeye/"><![CDATA[<p>As the lead developer at Tata 1mg, designing the Common Order Taking (COT) portal, I realized the critical need for analytics and performance metrics in high-efficiency retail stores, dark stores, and warehouses. When every last rupee counts towards profitability, understanding these metrics is imperative. So during my notice period i have decided to work on something called hawkeye ( i watch cricket a lot and hence the name )</p>

<h2 id="introducing-hawk-eye">Introducing HaWk Eye</h2>

<p>The idea of HaWk Eye (yes, I wanted to spell it with a capital H and W) was born from this need. I envision it as a computer vision software that can evolve into a great SaaS product. It has the potential to be specialized for very specific use cases like warehouses and retail stores.</p>

<h2 id="what-to-expect">What to Expect</h2>

<p>I‚Äôll keep updating this blog with my progress on HaWk Eye, detailing what I‚Äôm doing and how I‚Äôm going about it. For anyone reading this, I aim to keep everything organized in one specific folder. This page will serve as the root for all future updates.</p>

<p>As I pen down this header, I am excited and eager to code. Happy reading!</p>

<h2 id="progress">Progress</h2>
<p>-<a href="../building_hawkeye/week1">Week 1: Core Functionality Development</a></p>]]></content><author><name>Yash</name><email>yashpathania704@gmail.com</email></author><category term="Computer Vision" /><category term="Python" /><summary type="html"><![CDATA[As the lead developer at Tata 1mg, designing the Common Order Taking (COT) portal, I realized the critical need for analytics and performance metrics in high-efficiency retail stores, dark stores, and warehouses. When every last rupee counts towards profitability, understanding these metrics is imperative. So during my notice period i have decided to work on something called hawkeye ( i watch cricket a lot and hence the name )]]></summary></entry><entry><title type="html">Week 1: Core Functionality Development</title><link href="http://localhost:4000/posts/building_hawkeye/week1/" rel="alternate" type="text/html" title="Week 1: Core Functionality Development" /><published>2024-07-14T00:00:00+01:00</published><updated>2024-07-14T00:00:00+01:00</updated><id>http://localhost:4000/posts/building_hawkeye/week1</id><content type="html" xml:base="http://localhost:4000/posts/building_hawkeye/week1/"><![CDATA[<p>Day 1-7: Project Setup and Initial Development</p>

<h2 id="day-1-setting-up-hawkeye">Day 1: Setting Up Hawkeye</h2>

<p><em>13 July 2024</em>
Hey everyone, on day one my basic aim is to research what I am going to pen down, jot down my feelings, and think about this. I am thinking of using FastAPI for the backend because I have never used it before, and I consider it should not be too heavy when it comes to backend development.</p>

<h3 id="frameworks-and-tools">Frameworks and Tools</h3>
<ul>
  <li>Backend: FastAPI for the API</li>
  <li>Frontend: Streamlit (I chose Streamlit because it looks good, and for an MVP, I believe frontend aesthetics are not critical. We can focus on backend functionality and scale up to something more robust and visually appealing later on.)</li>
  <li>Database: PostgreSQL or SQLite for storing video data and analysis results (I prefer PostgreSQL due to my familiarity with the Tortoise wrapper, which should facilitate better interaction. I still need to finalize the database design.)</li>
  <li>Cloud Storage: AWS S3 (I can set up a local stack to mirror AWS S3)</li>
  <li>Deployment: Docker for containerization, AWS EC2 (local stack supremacy)</li>
</ul>

<p>Today, I made progress by setting up my GitHub repository, pushing my requirements, and establishing a basic file structure. I faced some challenges because I just bought a new MacBook Pro M3, so I had to install Python and everything from scratch, but it was fairly easy. I‚Äôve also created a <code class="language-plaintext highlighter-rouge">requirements.txt</code> that will help in setting up and Dockerizing later. My next steps involve setting up the basic application.</p>

<h2 id="github-repo-link-hawkeye-github-repo">GitHub repo link: <a href="https://github.com/yash-singh-pathania/Hawkeye">Hawkeye GitHub Repo</a></h2>

<h2 id="day-2-trying-my-hand-on-detection">Day 2: Trying My Hand On Detection</h2>

<p><em>14 July 2024</em>
This was day 2 of the project, and I was weirdly excited about it. I had to decide whether to try detecting people and tracking them or do other things that would make my life better in the long run, like setting up pre-commit hooks, maybe setting up a linter, and so on. But being a developer at heart, I went the detection route and set up basic detection for people and tracking them across the retail floor.</p>

<p>I am still looking for some model that does this better because right now I find myself stuck on how to make this MVP. My idea is simple:</p>

<p>If I can process a pre-recorded video in 80% of the time of the full video‚Äîsay if the video is 10 minutes and I can process it in 8 minutes‚ÄîI should be able to do this virtually live, which is my aim in the long run. Right now, I am having difficulties setting up Streamlit, sending it the tracking data and the video, and putting it all together. I think I‚Äôll do it together; let‚Äôs see.</p>

<p>Today‚Äôs commit <a href="https://github.com/Yash-Git-Hub/Hawkeye/compare/481a8eb6e00e15b6cb6493122557534439894e95...cf40098133a17c5438040cadca662e35221db8e7">commit</a></p>

<hr />]]></content><author><name>Yash</name><email>yashpathania704@gmail.com</email></author><category term="Computer Vision" /><category term="Python" /><category term="Building Hawkeye" /><summary type="html"><![CDATA[Day 1-7: Project Setup and Initial Development]]></summary></entry><entry><title type="html">Leet Code Subpage 2</title><link href="http://localhost:4000/posts/leetcode/subpage2/" rel="alternate" type="text/html" title="Leet Code Subpage 2" /><published>2024-07-14T00:00:00+01:00</published><updated>2024-07-14T00:00:00+01:00</updated><id>http://localhost:4000/posts/leetcode/subpage2</id><content type="html" xml:base="http://localhost:4000/posts/leetcode/subpage2/"><![CDATA[<p>This is the second subpage for the LeetCode Fun Questions blog post. Here, I will cover more advanced problems and techniques.</p>]]></content><author><name>Yash</name><email>yashpathania704@gmail.com</email></author><category term="leetcode" /><summary type="html"><![CDATA[This is the second subpage for the LeetCode Fun Questions blog post. Here, I will cover more advanced problems and techniques.]]></summary></entry><entry><title type="html">Leet Code Fun Questions</title><link href="http://localhost:4000/posts/leetcode/" rel="alternate" type="text/html" title="Leet Code Fun Questions" /><published>2024-07-14T00:00:00+01:00</published><updated>2024-07-14T00:00:00+01:00</updated><id>http://localhost:4000/posts/leetcode</id><content type="html" xml:base="http://localhost:4000/posts/leetcode/"><![CDATA[<p>This is the place where I write about the LeetCode questions I find interesting. I rarely do LeetCode, but I need to brush up on my DSA and competitive coding skills before my master‚Äôs, so I‚Äôll be blogging about that here.</p>

<p>Check out these subpages for more detailed discussions:</p>
<ul>
  <li><a href="../leetcode/1518">Bottle Exchange</a></li>
  <li><a href="../leetcode/subpage2">Subpage 2</a></li>
</ul>]]></content><author><name>Yash</name><email>yashpathania704@gmail.com</email></author><category term="Leetcode" /><category term="Competitive Coding" /><summary type="html"><![CDATA[This is the place where I write about the LeetCode questions I find interesting. I rarely do LeetCode, but I need to brush up on my DSA and competitive coding skills before my master‚Äôs, so I‚Äôll be blogging about that here.]]></summary></entry><entry><title type="html">Moving From ruby 2.6 -&amp;gt; Ruby 3.2 Yjit</title><link href="http://localhost:4000/posts/yjit_upgrade" rel="alternate" type="text/html" title="Moving From ruby 2.6 -&amp;gt; Ruby 3.2 Yjit" /><published>2024-04-07T00:00:00+01:00</published><updated>2024-04-07T00:00:00+01:00</updated><id>http://localhost:4000/posts/future-post</id><content type="html" xml:base="http://localhost:4000/posts/yjit_upgrade"><![CDATA[<p>This post will show up by default. To disable scheduling of future posts, edit <code class="language-plaintext highlighter-rouge">config.yml</code> and set <code class="language-plaintext highlighter-rouge">future: false</code>.</p>]]></content><author><name>Yash</name><email>yashpathania704@gmail.com</email></author><category term="ruby" /><category term="engineering" /><category term="yjit" /><summary type="html"><![CDATA[This post will show up by default. To disable scheduling of future posts, edit config.yml and set future: false.]]></summary></entry><entry><title type="html">Postgres Sequence Item Go Up Even If Object Creation Fails</title><link href="http://localhost:4000/posts/next_val_postgres/" rel="alternate" type="text/html" title="Postgres Sequence Item Go Up Even If Object Creation Fails" /><published>2024-03-25T00:00:00+00:00</published><updated>2024-03-25T00:00:00+00:00</updated><id>http://localhost:4000/posts/next_val_postgres</id><content type="html" xml:base="http://localhost:4000/posts/next_val_postgres/"><![CDATA[<p>Today while working  on a ror project we run into an abrupt issue wiht transaction where even if the transaction was not being processed the id was increasing leading to a failure in a backgroud sidekiq worker this lead me to this stackover flow post</p>

<p><a href="https://stackoverflow.com/questions/50650010/why-does-postgres-sequence-item-go-up-even-if-object-creation-fails">Post</a>
Which then led me in rabit hole that i have tried to pen to the best of my abilities below :-&gt;</p>

<h1 id="postgresql-sequences-unexpected-increments">PostgreSQL Sequences: Unexpected Increments?</h1>

<p>Imagine you‚Äôre working with a PostgreSQL database, and you‚Äôve set up a sequence to auto-increment primary keys for your <code class="language-plaintext highlighter-rouge">Client</code> model. Everything seems fine until you encounter a puzzling issue: the sequence value jumps up even when a client creation fails.</p>

<h2 id="the-mystery-unveiled">The Mystery Unveiled</h2>

<p>In PostgreSQL, sequences are special objects used to generate unique identifiers, commonly for primary key fields. When you create a new record, PostgreSQL automatically fetches the next value from the sequence and assigns it to the primary key column.</p>

<h3 id="example-scenario">Example Scenario</h3>

<p>Let‚Äôs say your sequence currently stands at 262. You attempt to create a new client, but due to a unique constraint violation (perhaps someone manually set a primary key, which PostgreSQL sequences ignore), the creation fails. Oddly enough, upon rechecking the sequence, you find it‚Äôs incremented to 263, despite no new client being added to the database.</p>

<h3 id="why-does-this-happen">Why Does This Happen?</h3>

<p>PostgreSQL‚Äôs sequence mechanism operates independently of transaction rollbacks. When you call <code class="language-plaintext highlighter-rouge">nextval</code> on a sequence (implicitly done when a new record is inserted), it advances the sequence whether or not the transaction succeeds or fails. This design ensures each session receives a unique sequence value, even if multiple sessions are running concurrently.</p>

<h3 id="consider-the-consequences">Consider the Consequences</h3>

<p>This behavior can lead to unexpected scenarios if not handled carefully. For instance, if your application logic relies on sequential numbering for auditing or reporting purposes, gaps might appear if transactions fail after fetching a sequence value. These gaps are harmless but can be surprising if not anticipated.</p>

<h3 id="best-practices">Best Practices</h3>

<p>To avoid issues:</p>
<ul>
  <li><strong>Avoid Manually Setting Primary Keys:</strong> Let PostgreSQL manage sequence values automatically.</li>
  <li><strong>Handle Unique Constraints Gracefully:</strong> Ensure your application handles unique constraint violations gracefully to prevent unnecessary gaps in sequence usage.</li>
</ul>

<h2 id="visualizing-postgresql-sequence-behavior">Visualizing PostgreSQL Sequence Behavior</h2>

<p>To illustrate, here‚Äôs a table summarizing how PostgreSQL sequences behave:</p>

<table>
  <thead>
    <tr>
      <th>Action</th>
      <th>Sequence Value Before</th>
      <th>Sequence Value After</th>
      <th>Transaction Outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Attempt to Create Client</td>
      <td>262</td>
      <td>262 (if creation fails)</td>
      <td>Transaction fails, no new client added</td>
    </tr>
    <tr>
      <td>Retry Creation</td>
      <td>262</td>
      <td>263 (if creation succeeds)</td>
      <td>Transaction succeeds, new client added</td>
    </tr>
    <tr>
      <td>Query Sequence</td>
      <td>263</td>
      <td>263</td>
      <td>Query reflects latest sequence value</td>
    </tr>
  </tbody>
</table>

<h2 id="conclusion">Conclusion</h2>

<p>Understanding PostgreSQL sequences is crucial for maintaining data integrity and application reliability. While the behavior of sequence incrementation on failed transactions might seem counterintuitive at first, it ensures robustness and scalability in multi-session environments.</p>

<p>So, the next time you encounter an unexpected sequence increment in PostgreSQL, remember: it‚Äôs all part of PostgreSQL‚Äôs design to maintain transactional integrity and support concurrent operations seamlessly.</p>

<p>By grasping these nuances, you can navigate PostgreSQL‚Äôs sequence behavior more effectively, ensuring your applications perform reliably even under challenging conditions. Understanding these mechanics not only enhances your troubleshooting skills but also empowers you to design more resilient database architectures.</p>]]></content><author><name>Yash</name><email>yashpathania704@gmail.com</email></author><category term="Postgres" /><summary type="html"><![CDATA[Today while working on a ror project we run into an abrupt issue wiht transaction where even if the transaction was not being processed the id was increasing leading to a failure in a backgroud sidekiq worker this lead me to this stackover flow post]]></summary></entry><entry><title type="html">1518-Water Bottles</title><link href="http://localhost:4000/posts/leetcode/1518/" rel="alternate" type="text/html" title="1518-Water Bottles" /><published>2022-08-14T00:00:00+01:00</published><updated>2022-08-14T00:00:00+01:00</updated><id>http://localhost:4000/posts/leetcode/water_bottles</id><content type="html" xml:base="http://localhost:4000/posts/leetcode/1518/"><![CDATA[<h2 id="optimized-bottle-exchange">Optimized Bottle Exchange</h2>
<p>Even though this is an easy solution  i absoluetly loved this elegant solution that i thoough of.</p>
<h3 id="approach">Approach</h3>

<p>In this optimized approach, we use math to gulp down as many bottles as possible before exchanging them for full ones. By maximizing each exchange, we ensure efficiency.</p>

<h3 id="steps">Steps</h3>

<ol>
  <li>Start with a count of empty bottles consumed set to zero.</li>
  <li>Continue until the number of full bottles is greater than the exchange rate:
    <ul>
      <li>Calculate the maximum number of full bottles we can gulp down in one go.</li>
      <li>Add these to our consumed count.</li>
      <li>Subtract the corresponding empty bottles from the total.</li>
      <li>Exchange the empty bottles for new full ones.</li>
    </ul>
  </li>
  <li>Return the total number of bottles consumed, including any remaining full bottles.</li>
</ol>

<h3 id="implementation">Implementation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">num_water_bottles</span><span class="p">(</span><span class="n">numBottles</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">numExchange</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">consumedBottles</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">numBottles</span> <span class="o">&gt;=</span> <span class="n">numExchange</span><span class="p">:</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">numBottles</span> <span class="o">//</span> <span class="n">numExchange</span>
        <span class="n">consumedBottles</span> <span class="o">+=</span> <span class="n">numExchange</span> <span class="o">*</span> <span class="n">K</span>
        <span class="n">numBottles</span> <span class="o">=</span> <span class="n">numBottles</span> <span class="o">-</span> <span class="n">numExchange</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">K</span>
    <span class="k">return</span> <span class="n">consumedBottles</span> <span class="o">+</span> <span class="n">numBottles</span>
</code></pre></div></div>
<h3 id="complexity">Complexity</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Time: Efficient, logarithmic relative to the number of bottles.
- Space: Minimal, only requires a few variables.
</code></pre></div></div>

<h3 id="question-pattern-resembles">Question Pattern Resembles</h3>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> - Greedy Approach:  You could say it is greedy but i wouldnt classify it as such overall i cant pinpoint it to any particular pattern. 
</code></pre></div></div>

<p>That reminds me Enjoy drinking responsibly and efficiently!</p>]]></content><author><name>Yash</name><email>yashpathania704@gmail.com</email></author><category term="leetcode" /><summary type="html"><![CDATA[Optimized Bottle Exchange Even though this is an easy solution i absoluetly loved this elegant solution that i thoough of. Approach]]></summary></entry><entry><title type="html">Predicting Stock Prices with Sentiment Analysis üìàüòÑ</title><link href="http://localhost:4000/posts/Lstm-Stock/" rel="alternate" type="text/html" title="Predicting Stock Prices with Sentiment Analysis üìàüòÑ" /><published>2022-06-14T00:00:00+01:00</published><updated>2022-06-14T00:00:00+01:00</updated><id>http://localhost:4000/posts/lstm</id><content type="html" xml:base="http://localhost:4000/posts/Lstm-Stock/"><![CDATA[<p>The ideation of this project came from our capstone project under <a href="https://csed.thapar.edu/facultydetails/OTYx">Dr. Seema Bhardwaj</a>. We aimed to create an ML model predictor to help protect my friends from dubious websites and irregular patterns. Simple Stock, the product we developed, focused on prediction and teaching students about stock market trends. This blog deals with the prediction part and our approach to it. If you‚Äôre interested in learning more about the product here is much lengthier <a href="https://drive.google.com/file/d/1-iVwzWASrF0pgtrxj69C8T6OpCDish6C/view?usp=share_link"><code class="language-plaintext highlighter-rouge">breif</code></a> about it .</p>

<p>In this blog post, I‚Äôll take you through one of my exciting data science projects where I used sentiment analysis of news articles to predict stock prices using an LSTM model. Let‚Äôs dive in! üöÄ</p>

<h2 id="introduction">Introduction</h2>

<p>Predicting stock prices is a challenging task due to the numerous factors influencing them. In this project, we will use the VADER sentiment analysis model to analyze news articles and incorporate their sentiment scores as features in our stock price prediction model. Our model will use an LSTM (Long Short-Term Memory) architecture to make predictions based on historical stock prices and news sentiments.</p>

<h2 id="step-by-step-guide">Step-by-Step Guide</h2>

<h3 id="step-1-data-collection-">Step 1: Data Collection üìù</h3>

<p>First, we need to collect historical stock prices and news articles. For this project, I used data from the New York Times and historical stock prices of the top 10 companies in the S&amp;P 500 index.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">yfinance</span> <span class="k">as</span> <span class="n">yf</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># List of top 10 S&amp;P 500 companies
</span><span class="n">companies</span> <span class="o">=</span> <span class="p">[</span><span class="s">"AAPL"</span><span class="p">,</span> <span class="s">"MSFT"</span><span class="p">,</span> <span class="s">"AMZN"</span><span class="p">,</span> <span class="s">"GOOGL"</span><span class="p">,</span> <span class="s">"FB"</span><span class="p">,</span> <span class="s">"TSLA"</span><span class="p">,</span> <span class="s">"BRK-B"</span><span class="p">,</span> <span class="s">"JPM"</span><span class="p">,</span> <span class="s">"JNJ"</span><span class="p">,</span> <span class="s">"V"</span><span class="p">]</span>

<span class="c1"># Download historical stock prices
</span><span class="n">stock_data</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">company</span> <span class="ow">in</span> <span class="n">companies</span><span class="p">:</span>
    <span class="n">stock_data</span><span class="p">[</span><span class="n">company</span><span class="p">]</span> <span class="o">=</span> <span class="n">yf</span><span class="p">.</span><span class="n">download</span><span class="p">(</span><span class="n">company</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="s">"2015-01-01"</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s">"2020-12-31"</span><span class="p">)</span>
    
<span class="c1"># Convert to DataFrame
</span><span class="n">stock_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span><span class="n">stock_data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">stock_prices</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">"stock_prices.csv"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="step-2-sentiment-analysis-with-vader-">Step 2: Sentiment Analysis with VADER üòÉüòû</h3>

<p>Next, we‚Äôll use the VADER sentiment analysis model to analyze the sentiment of news articles. VADER is a pre-trained model specifically designed for sentiment analysis of social media texts.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">vaderSentiment.vaderSentiment</span> <span class="kn">import</span> <span class="n">SentimentIntensityAnalyzer</span>
<span class="kn">import</span> <span class="nn">requests</span>

<span class="n">analyzer</span> <span class="o">=</span> <span class="n">SentimentIntensityAnalyzer</span><span class="p">()</span>

<span class="c1"># Function to fetch and analyze news sentiment
</span><span class="k">def</span> <span class="nf">fetch_news_sentiment</span><span class="p">(</span><span class="n">company</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"https://api.nytimes.com/svc/search/v2/articlesearch.json?q=</span><span class="si">{</span><span class="n">company</span><span class="si">}</span><span class="s">&amp;api-key=YOUR_API_KEY"</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="n">articles</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="n">json</span><span class="p">()[</span><span class="s">"response"</span><span class="p">][</span><span class="s">"docs"</span><span class="p">]</span>
    
    <span class="n">sentiments</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">article</span> <span class="ow">in</span> <span class="n">articles</span><span class="p">:</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">article</span><span class="p">[</span><span class="s">"lead_paragraph"</span><span class="p">]</span>
        <span class="n">sentiment</span> <span class="o">=</span> <span class="n">analyzer</span><span class="p">.</span><span class="n">polarity_scores</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">sentiments</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sentiment</span><span class="p">[</span><span class="s">"compound"</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">sentiments</span>

<span class="c1"># Analyze sentiments for each company
</span><span class="n">news_sentiments</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">company</span> <span class="ow">in</span> <span class="n">companies</span><span class="p">:</span>
    <span class="n">news_sentiments</span><span class="p">[</span><span class="n">company</span><span class="p">]</span> <span class="o">=</span> <span class="n">fetch_news_sentiment</span><span class="p">(</span><span class="n">company</span><span class="p">)</span>

<span class="c1"># Convert to DataFrame
</span><span class="n">sentiment_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">news_sentiments</span><span class="p">)</span>
<span class="n">sentiment_df</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">"news_sentiments.csv"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="step-3-data-preparation-">Step 3: Data Preparation üìä</h3>

<p>We need to prepare our data by merging the stock prices and news sentiments. We‚Äôll also normalize the data to ensure consistency.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load data
</span><span class="n">stock_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"stock_prices.csv"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">sentiments</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"news_sentiments.csv"</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Normalize stock prices
</span><span class="n">stock_prices_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">stock_prices</span> <span class="o">-</span> <span class="n">stock_prices</span><span class="p">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="n">stock_prices</span><span class="p">.</span><span class="n">std</span><span class="p">()</span>

<span class="c1"># Merge data
</span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">stock_prices_norm</span><span class="p">,</span> <span class="n">sentiments</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">data</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="step-4-building-the-lstm-model-">Step 4: Building the LSTM Model üß†</h3>
<p>We‚Äôll build our LSTM model using TensorFlow/Keras. The model will have an input layer, two LSTM layers, and a dense output layer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Prepare data for LSTM
</span><span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">window_size</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="n">window_size</span><span class="p">):</span>
        <span class="n">X</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">window_size</span><span class="p">])</span>
        <span class="n">y</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">window_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="n">window_size</span> <span class="o">=</span> <span class="mi">60</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">values</span><span class="p">,</span> <span class="n">window_size</span><span class="p">)</span>

<span class="c1"># Split data
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Build LSTM model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
    <span class="n">LSTM</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">window_size</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])),</span>
    <span class="n">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>

<span class="c1"># Train model
</span><span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="step-5-evaluating-the-model-">Step 5: Evaluating the Model üìâ</h3>
<p>We‚Äôll evaluate our model using the Root Mean Squared Error (RMSE) metric to assess its accuracy.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="c1"># Predict
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"RMSE: </span><span class="si">{</span><span class="n">rmse</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="conclusion-">Conclusion üéâ</h3>
<p>In this project, we successfully used sentiment analysis of news articles to improve stock price prediction with an LSTM model. By combining historical stock prices and sentiment scores, our model achieved some pretty impressive results. But hey, there‚Äôs always room for improvement! So, don‚Äôt hesitate to experiment with different model architectures and datasets. Happy coding! üòä</p>

<p>Oh, and if you want to check out the initial stages and thought process behind this blog, check out these <a href="https://docs.google.com/presentation/d/1p5pjDrXCANU300sAD91NlZbE9xU09SUX/edit?usp=share_link&amp;ouid=112246221369441046993&amp;rtpof=true&amp;sd=true">slides</a> from back when I was in 3rd year. Do remember, I‚Äôve significantly improved at making presentations‚Äîoops, I mean coding‚Äîsince then!</p>]]></content><author><name>Yash</name><email>yashpathania704@gmail.com</email></author><category term="Lstm" /><category term="DataScience" /><category term="Python" /><summary type="html"><![CDATA[The ideation of this project came from our capstone project under Dr. Seema Bhardwaj. We aimed to create an ML model predictor to help protect my friends from dubious websites and irregular patterns. Simple Stock, the product we developed, focused on prediction and teaching students about stock market trends. This blog deals with the prediction part and our approach to it. If you‚Äôre interested in learning more about the product here is much lengthier breif about it .]]></summary></entry></feed>